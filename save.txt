# details = e.find('div', class_='base-search-card__info')
            # job_title_element = details.find('h3', class_='base-search-card__title')
            # job_role = job_title_element.text.strip() if job_title_element else ''
            # company_element = e.find('h4', class_='base-search-card__subtitle')
            # company_name = company_element.text.strip() if company_element else ''
            # try:
            #     company_logo = e.find('img')['data-delayed-url']
            # except:
            #     company_logo = ''
            
            # posted_date_element = e.find('time', class_='job-search-card__listdate--new')
            # posted_date = posted_date_element['datetime'] if posted_date_element else ''
            # location_element = e.find('span', class_='job-search-card__location')
            # location = location_element.text.strip() if location_element else ''
            # time.sleep(3)
            # response = requests.get(joblink)
            # time.sleep(1)
            # if response.status_code == 200:
            #     soup = BeautifulSoup(response.content, 'html.parser')
                
            #     full_description_element = soup.find('div', class_='show-more-less-html__markup')
            #     full_description = str(full_description_element) if full_description_element else "" 
            #     if full_description not in jobs_descriptions:
            #         jobs_descriptions.add(full_description)
            #         if 'united' in location.lower():
            #             location = location.replace(', United States', '')
                    
            #         city = shortregion = state = ''
            #         location_split = location.split(',')

            #         if 'remote' in location.lower() or 'remote' in job_role.lower():
            #             city = shortregion = state = location = ''
                    
            #         elif len(location_split) == 1 and 'United States' not in location_split[-1]:
            #             state = location_split[0]
            #             location = city = shortregion = ''
            #         elif len(location_split[-1].strip()) == 2:
            #             shortregion = location_split[-1]
            #             city = location_split[0]
            #             location = f'{city}, {shortregion}'
            #             state = ''
            #         else:
            #             state = location_split[-1]
            #             city = location_split[0]
            #             location = f'{city}, {state}'
            #             shortregion = ''  
            #         if state.lower() == 'united states':
            #             state = ''
            #         try:
            #             job_salary_elm = soup.find('div', class_='compensation__salary').text.strip()
            #             if '/' in job_salary_elm:
            #                 salary_nums = job_salary_elm.split('-')
            #                 job_salary_elm = ' - '.join(sal.split('/')[0].strip() for sal in salary_nums)
            #                 salary_type = salary_nums[0].split('/')[1].strip()
            #         except:
            #             job_salary_elm = ''
            #             salary_type = ''
                    
            #         employment_type = ""
            #         job_criteria_items = soup.find_all('li', class_='description__job-criteria-item')
            #         for item in job_criteria_items:
            #             header = item.find('h3', class_='description__job-criteria-subheader').text.strip()
            #             if header == "Employment type":
            #                 employment_type = item.find('span', class_='description__job-criteria-text').text.strip()
            #                 break
            #         contact_emails = extract_emails(full_description)
            #         contact_email = contact_emails[0] if contact_emails else ""
            #         if joblink:
            #             job_data = {
            #                 'title': job_role,
            #                 'companyname': company_name,
            #                 'joblink': joblink,
            #                 'location': location,
            #                 'date_posted': posted_date,
            #                 'state': state,
            #                 'shortregion': shortregion,
            #                 'city': city,
            #                 'jobtype': employment_type,
            #                 'jobdescription': full_description,
            #                 'jobid': jobid,
            #                 'salaryrange': job_salary_elm,
            #                 'salarytype': salary_type,
            #                 'email': contact_email,
            #                 'contact_name': "",
            #                 'source_name': "linkedin",
            #                 'company_logo': company_logo,
            #                 "experience": "",
            #                 "workpermit": "",
            #                 "skills": ""
            #             }
            #             save_to_csv([job_data], file_save_path)
            #             print(f"Scraped job: {job_role} at {company_name}")
            #             print(job_data)
            #             time.sleep(3)
        
        except Exception as e: 
            logger.error("An error occurred--", exc_info=True)
            print(e)